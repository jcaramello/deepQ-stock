\chapter{Evaluación y Desempeño}


\section{Introducción}

Durante este capitulo nos dedicaremos a analizar el desempeño de nuestro agente. Para ello definiremos una serie de configuraciones, donde haremos variar algunos de los diferentes parámetros de ejecución y usaremos cada una de estas configuraciones sobre diferentes compañías, para luego tratar de evaluar como dichos parámetros, afectan o no, en el desempeño del agente. Lo interesante sera observar tanto los patrones de compra y venta ejecutados por el agente durante toda la simulación, como así también las ganancias obtenidas en cada configuración.

\section{Configuraciones}

La siguientes configuraciones(Cuadro \ref{tabla:cap5:1}) serán ejecutas múltiples veces sobre diferente compañías. El objetivo principal de estas configuraciones es tratar de ver como se comporta el algoritmo bajo diferentes variaciones. 

En principio hemos considerado 3 configuración que según creemos, cada una sera mejor que la siguiente, partiendo desde la  mas optima(moe), a una intermedia(lenny) y por ultimo la que peor debería performar, ralp, pues se acerca mucho a un agente que toma decisiones en forma aleatoria. 

\begin{table}[]
	\centering
	\caption{Configuraciones de los agentes}
	\label{tabla:cap5:1}
	\begin{tabular}{llll}
		Nombre                     & moe & lenny & ralph \\
		$\varepsilon$-greedy       & 0.1 & 0.3   & 1     \\
		learnig rate    $\alpha$   & 0.3 & 0.3   & 0.3   \\
		discount factor $\gamma$   & 0.8 & 0.5   & 0.1   \\
		mini batch                 & 50  & 100   & 5    \\
		memory replay      		   & 500 & 1000  & 10     \\
		hidden layers     		   & 4   & 8     & 2     \\
		neurons per layer  		   & 16  & 32    & 4    
	\end{tabular}
\end{table}

Todas las configuraciones se ejecutaran sobre dos compañías cuyos papeles tienen comportamientos completamente diferentes.
Una es Apple, donde claramente puede verse que el valor de las acciones generalmente se encuentra dentro de un tendencias alcista durante los 10 años de simulación y la otra es Microsoft la cual posee un comportamiento mas estable, con tramos alcistas y bajistas y tramos donde el papel lateraliza. Las decisiones del agente se mostraran con burbujas de color azul para las compras, y burbujas de color amarillo para las ventas, a media que transcurran los días, se irán marcado cada vela, con una burbuja indicando la acción, para poder simplificar la visualización del gráfico, hemos decidido, no marcar con ningún marcador, los decisiones de esperar.

\section{Resultados}

\subsection{moe}
Estos son algunos de los resultados de la primera ejecución de moe sobre ambas compañías.

\begin{figure}[h!]
	\includegraphics[scale=0.5]{imagenes/moe_appl_ex1_1_summary.png}
	\caption{Resultados de moe sobre apple}
	\label{fig:cap5:1}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.45]{imagenes/moe_msft_ex1_1_summary.png}
	\caption{Resultados de moe sobre microsoft}
	\label{fig:cap5:2}
\end{figure}

En este caso, el agente logro mejores resultados con Apple(Figura \ref{fig:cap5:1}), que con Microsoft (Figura \ref{fig:cap5:2}) probablemente producto de la propia característica de un papel generalmente alcista.
Sin embargo, observemos que sucede durante los primeros días de la simulación.

\begin{figure}[h!]
	\includegraphics[scale=0.5]{imagenes/moe_appl_ex1_1.png}
	\caption{Primeros días de trading sobre Apple}
	\label{fig:cap5:3}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.45]{imagenes/moe_msft_ex1_1.png}
	\caption{Primeros días de trading sobre Microsoft}
	\label{fig:cap5:4}
\end{figure}

Claramente puede observarse que el agente se predispone mayoritariamente a realizar siempre la misma acción, en de Apple decide vender (Figura \ref{fig:cap5:3}), y en el caso de Microsoft (Figura \ref{fig:cap5:4}) comprar.
Probablemente esto este determinado por la elección de la primera acción, pues sera la que en principio mas se repita, y de la que mayor conocimiento tiene sobre su recompensa futura. A media que vaya experimentado en forma aleatoria otras acciones, gracias a la policy $\varepsilon$-greedy, deberíamos esperar ver un cambio en el patrón de decisiones.
Veamos ahora que sucede con los últimos días de trading:

\begin{figure}[h!]
	\includegraphics[scale=0.7]{imagenes/moe_appl_ex1_2.png}
	\caption{Últimos días de trading sobre Apple}
	\label{fig:cap5:5}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.7]{imagenes/moe_msft_ex1_2.png}
	\caption{Últimos días de trading sobre Microsoft}
	\label{fig:cap5:6}
\end{figure}

En ambos casos, puede observarse una mejoría, tal vez, en el caso de Apple sea mas notoria (Figura \ref{fig:cap5:5}), dado que hasta puede observarse que durante varios días, el agente solo decide esperar, y especular esperando que el precio suba para vender luego de varios días gran parte de sus acciones a un precio mucho mayor al que las compro.
Mientras que en el caso de Microsoft (Figura \ref{fig:cap5:6}), no es tan claro, pero sin embargo, puede verse que cada 4 o  5 días, el agente compra, para luego vender. Es importante notar aquí, que dado el esquema de entrada y salida que se decidió adoptar, los días en que compra, comprar mucho mas de lo que luego vende en los días siguientes.

Un aspecto interesante que surgió durante la investigación, fue preguntarnos que sucedería si volviéramos a ejecutar el agente ya entrenado, es decir, reutilizando la matriz de pesos $\theta$ de su red neuronal, ¿Podría el agente mejorar su performance?.

Estos son los resultados que obtuvimos para moe (Figuras \ref{fig:cap5:7} y \ref{fig:cap5:8}), luego de 6 simulaciones:

\begin{figure}[h!]
	\includegraphics[scale=0.7]{imagenes/moe_results_1.png}
	\caption{Promedio del rate de ganancias finales}
	\label{fig:cap5:7}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.7]{imagenes/moe_results_2.png}
	\caption{Evolución de ganancias}
	\label{fig:cap5:8}
\end{figure}

Mirando los gráficos resultantes, no pareciera a ver una mejoría significativa, aunque si podemos afirmar que los agentes en general, repitieron su performance. Sin embargo pudimos observar una notoria modificación en el patrón de decisiones del agente:

\begin{figure}[h!]
	\includegraphics[scale=0.4]{imagenes/moe_appl_ex6_1.png}
	\caption{Resultados de moe luego de 6 ejecuciones}
	\label{fig:cap5:9}
\end{figure}

Claramente podemos notar (Figura \ref{fig:cap5:9}), que el agente toma decisiones de compra y/o venta mucho mas espaciadas en el tiempo, a comparación de las primeras ejecuciones, donde el agente tendía a sobre operar en el mercado, y terminaba realizado compras y ventas diariamente. No podemos concluir si esto significa una mejora o no, aunque si es posible asociar este tipo de patrón a un comportamiento mucho mas especulativo y por lo tanto mas parecido al que veríamos en un inversor real.

\subsection{lenny}

Como podíamos esperar, efectivamente lenny, se desempeño peor que moe.
Sin embargo también podemos observar un cambio en el patrón de decisión a lo largo del tiempo (Figuras \ref{fig:cap5:10} y \ref{fig:cap5:11}).

\begin{figure}[h!]
	\includegraphics[scale=0.5]{imagenes/lenny_appl_ex1_1.png}
	\caption{Ejecución de lenny sobre Apple}
	\label{fig:cap5:10}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.5]{imagenes/lenny_msft_ex1_1.png}
	\caption{Ejecución de lenny sobre Microsoft}
	\label{fig:cap5:11}
\end{figure}


\subsection{ralph}
Por ultimo, nos queda analizar que sucedió con ralp, y nuevamente, se dio lo que esperamos, si bien los resultados en términos del rate de ganancias, no difieren tanto (Figuras \ref{fig:cap5:12} y \ref{fig:cap5:13}), si podemos observar que el patrón de decisiones del agente se mantiene en el tiempo, dado que efectivamente siempre elige aleatoriamente. También pudimos observar un error muy grande durante el entrenamiento de la QNetwork, y que en general, dicho error no se reduce, provocando la no convergencia de la función Q

\begin{figure}[h!]
	\includegraphics[scale=0.4]{imagenes/lenny_appl_ex1_1.png}
	\caption{Ejecución de ralph sobre Apple}
	\label{fig:cap5:12}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.4]{imagenes/lenny_msft_ex1_1.png}
	\caption{Ejecución de ralph sobre Microsoft}
	\label{fig:cap5:13}
\end{figure}

\section{Resultados Finales}

Por ultimo, podemos ver los resultados finales de toda las ejecuciones y de todos los agentes en dos gráficos. El primero, mostrar el promedio del rate de ganancias de cada agente (Figura \ref{fig:cap5:14}) y el segundo la evolución de las ganancias de cada agente (Figura \ref{fig:cap5:14}).

\begin{figure}[h!]
	\includegraphics[scale=0.8]{imagenes/resultadofinal1.png}
	\caption{Promedio de Ganancias}
	\label{fig:cap5:14}
\end{figure}

\begin{figure}[h!]
	\includegraphics[scale=0.8]{imagenes/resultadofinal2.png}
	\caption{Evolución de ganancias}
	\label{fig:cap5:15}
\end{figure}

Claramente podemos ver que las ganancias obtenidas no son las mejores, sin embargo, podemos observar una leve mejoría luego de las primeras 2 o 3 simulaciones y ademas, y tal vez, lo mas interesante, es que resulta claro que el agente puede repetir su desempeño, lo cual, nos permite pensar que si lográramos encontrar alguna configuración mejor, podríamos acercarnos a un rate de ganancias mucho mejor.
